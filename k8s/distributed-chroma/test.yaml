---
# Source: distributed-chroma/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: chroma
---
# Source: distributed-chroma/templates/compaction-service.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: compaction-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/logservice.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logservice-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/query-service.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: query-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/sysdb-service.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sysdb-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/compaction-service.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: compaction-service-config
  namespace: chroma
data:
  config.yaml: |
    # Default configuration for query and compaction service
    # In the long term, every service should have an entry in this file
    # and this can become the global configuration file for Chroma
    # for now we nest it in the worker directory
    
    query_service:
        my_ip: "10.244.0.9"
        my_port: 50051
        assignment_policy:
            RendezvousHashing:
                hasher: Murmur3
        memberlist_provider:
            CustomResource:
                kube_namespace: "chroma"
                memberlist_name: "query-service-memberlist"
                queue_size: 100
        sysdb:
            Grpc:
                host: "sysdb.chroma"
                port: 50051
        storage:
            S3:
                bucket: "chroma-storage"
                credentials: "Minio"
        log:
            Grpc:
                host: "logservice.chroma"
                port: 50051
        dispatcher:
            num_worker_threads: 4
            dispatcher_queue_size: 100
            worker_queue_size: 100
    
    compaction_service:
        my_ip: "10.244.0.9"
        my_port: 50051
        assignment_policy:
            RendezvousHashing:
                hasher: Murmur3
        memberlist_provider:
            CustomResource:
                kube_namespace: "chroma"
                memberlist_name: "compaction-service-memberlist"
                queue_size: 100
        sysdb:
            Grpc:
                host: "sysdb.chroma"
                port: 50051
        storage:
            S3:
                bucket: "chroma-storage"
                credentials: "Minio"
        log:
            Grpc:
                host: "logservice.chroma"
                port: 50051
        dispatcher:
            num_worker_threads: 4
            dispatcher_queue_size: 100
            worker_queue_size: 100
        compactor:
            compaction_manager_queue_size: 1000
            max_concurrent_jobs: 100
            compaction_interval_sec: 60
---
# Source: distributed-chroma/templates/compaction-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: compaction-service-memberlist-readerwriter
rules:
- apiGroups:
  - chroma.cluster
  resources:
  - memberlists
  verbs:
  - get
  - list
  - watch
  # TODO: FIX THIS LEAKY PERMISSION
  - create
  - update
  - patch
  - delete
---
# Source: distributed-chroma/templates/query-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: query-service-memberlist-readerwriter
rules:
- apiGroups:
  - chroma.cluster
  resources:
  - memberlists
  verbs:
  - get
  - list
  - watch
  # TODO: FIX THIS LEAKY PERMISSION
  - create
  - update
  - patch
  - delete
---
# Source: distributed-chroma/templates/compaction-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sysdb-compaction-service-memberlist-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: compaction-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: sysdb-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/compaction-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  # Awkward name, but this lets the compaction-service-serviceaccount read
  # the compaction-service-memberlist.
  name: compaction-service-compaction-service-memberlist-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: compaction-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: compaction-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/compaction-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: compaction-service-memberlist-readerwriter-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: compaction-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: default
  namespace: chroma
---
# Source: distributed-chroma/templates/query-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sysdb-query-service-memberlist-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: query-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: sysdb-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/query-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  # Awkward name, but this lets the query-service-serviceaccount read
  # the query-service-memberlist.
  name: query-service-query-service-memberlist-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: query-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: query-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/query-service-memberlist-cr.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: query-service-memberlist-readerwriter-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: query-service-memberlist-readerwriter
subjects:
- kind: ServiceAccount
  name: default
  namespace: chroma
---
# Source: distributed-chroma/templates/pod-watcher-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: chroma
  name: pod-watcher
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
# Source: distributed-chroma/templates/compaction-service.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: compaction-service-serviceaccount-rolebinding
  namespace: chroma
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-watcher
subjects:
- kind: ServiceAccount
  name: compaction-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/query-service.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: query-service-serviceaccount-rolebinding
  namespace: chroma
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-watcher
subjects:
- kind: ServiceAccount
  name: query-service-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/sysdb-service.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sysdb-serviceaccount-rolebinding
  namespace: chroma
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-watcher
subjects:
- kind: ServiceAccount
  name: sysdb-serviceaccount
  namespace: chroma
---
# Source: distributed-chroma/templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: chroma
spec:
  ports:
    - name: server-port
      port: 8000
      targetPort: 8000
  selector:
    app: frontend-service
  type: ClusterIP
---
# Source: distributed-chroma/templates/logservice.yaml
apiVersion: v1
kind: Service
metadata:
  name: logservice
  namespace: chroma
spec:
  ports:
    - name: grpc
      port: 50051
      targetPort: grpc
  selector:
    app: logservice
  type: ClusterIP
---
# Source: distributed-chroma/templates/query-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: query-service
  namespace: chroma
spec:
  ports:
    - name: query-service-server-port
      port: 50051
      targetPort: 50051
  selector:
    app: query-service-server
  type: ClusterIP
---
# Source: distributed-chroma/templates/sysdb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sysdb
  namespace: chroma
spec:
  ports:
    - name: grpc
      port: 50051
      targetPort: grpc
  selector:
    app: sysdb
  type: ClusterIP
---
# Source: distributed-chroma/templates/compaction-service.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: compaction-service
  namespace: chroma
spec:
  replicas: 1
  selector:
    matchLabels:
      app: compaction-service
  template:
    metadata:
      labels:
        app: compaction-service
        member-type: compaction-service
    spec:
      serviceAccountName: compaction-service-serviceaccount
      
      volumes:
        - name: compaction-service-config
          configMap:
            name: compaction-service-config
      
      containers:
        - name: compaction-service
          image: "local:compaction-service"
          imagePullPolicy: IfNotPresent
          
          volumeMounts:
            - name: compaction-service-config
              mountPath: /config/
          
          ports:
            - containerPort: 50051
          env:
            
            - name: CONFIG_PATH
              value: /config/config.yaml
            
            
            - name: CHROMA_COMPACTION_SERVICE__MY_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: "kubernetes.io/hostname"
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              member-type: compaction-service
      volumes:
        - name: chroma
          emptyDir: {}
---
# Source: distributed-chroma/templates/frontend-service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-service
  namespace: chroma
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend-service
  template:
    metadata:
      labels:
        app: frontend-service
    spec:
      containers:
        - name: frontend-service
          image: "local:frontend-service"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: chroma
              mountPath: /test
          env:
            - name: IS_PERSISTENT
              value: "TRUE"
            - name: CHROMA_PRODUCER_IMPL
              value: "chromadb.logservice.logservice.LogService"
            - name: CHROMA_CONSUMER_IMPL
              value: "chromadb.logservice.logservice.LogService"
            - name: CHROMA_SEGMENT_MANAGER_IMPL
              value: "chromadb.segment.impl.manager.distributed.DistributedSegmentManager"
            - name: ALLOW_RESET
              value: "TRUE"
            - name: CHROMA_SYSDB_IMPL
              value: "chromadb.db.impl.grpc.client.GrpcSysDB"
            - name: CHROMA_SERVER_GRPC_PORT
              value: "50051"
            - name: CHROMA_COORDINATOR_HOST
              value: "sysdb.chroma"
            - name: CHROMA_SERVER_AUTHN_PROVIDER
              
            - name: CHROMA_SERVER_AUTHZ_PROVIDER
              value: ""
            - name: CHROMA_MEMBERLIST_PROVIDER_IMPL
              value: "chromadb.segment.impl.distributed.segment_directory.CustomResourceMemberlistProvider"
            - name: CHROMA_LOGSERVICE_HOST
              value: "logservice.chroma"
            - name: CHROMA_LOGSERVICE_PORT
              value: "50051"

            - name: CHROMA_OTEL_COLLECTION_ENDPOINT
              value: "http://jaeger:4317"
            - name: CHROMA_OTEL_GRANULARITY
              value: all
            
      volumes:
        - name: chroma
          emptyDir: {}
---
# Source: distributed-chroma/templates/logservice.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logservice
  namespace: chroma
spec:
  replicas: 1
  selector:
    matchLabels:
      app: logservice
  template:
    metadata:
      labels:
        app: logservice
    spec:
      serviceAccountName: logservice-serviceaccount
      containers:
        - env:
            
            - name: OPTL_TRACING_ENDPOINT
              # TODO properly use flow control here to check which type of value we need.

              value: "http://jaeger:4317"
            
          image: "local:log-service"
          imagePullPolicy: IfNotPresent
          name: logservice
          ports:
            - containerPort: 50051
              name: grpc
---
# Source: distributed-chroma/templates/query-service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: query-service
  namespace: chroma
spec:
  replicas: 2
  selector:
    matchLabels:
      app: query-service
  template:
    metadata:
      labels:
        app: query-service
        member-type: query-service
    spec:
      serviceAccountName: query-service-serviceaccount
      containers:
        - name: query-service
          image: "local:query-service"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 50051
          volumeMounts:
            - name: chroma
              mountPath: /index_data
          env:
            
            - name: CHROMA_QUERY_SERVICE__MY_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: "kubernetes.io/hostname"
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              member-type: query-service
      volumes:
        - name: chroma
          emptyDir: {}
---
# Source: distributed-chroma/templates/sysdb-service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sysdb
  namespace: chroma
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sysdb
  template:
    metadata:
      labels:
        app: sysdb
    spec:
      serviceAccountName: sysdb-serviceaccount
      containers:
        - command:
            - "/bin/sh"
            - "-c"
            # This has to be one line to be passed into the `exec` env correctly. I truly could not tell you why.
            - coordinator coordinator 
          env:
            
            - name: OPTL_TRACING_ENDPOINT
              # TODO properly use flow control here to check which type of value we need.

              value: "http://jaeger:4317"
            
          image: "local:sysdb"
          imagePullPolicy: IfNotPresent
          name: sysdb
          ports:
            - containerPort: 50051
              name: grpc
---
# Source: distributed-chroma/templates/log-migration.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: logservice-migration
  namespace: chroma
spec:
  template:
    metadata:
      labels:
        app: logservice-migration
    spec:
      restartPolicy: OnFailure
      containers:
        - args: ['migrate', 'apply', '--env','prod']
          image: "local:logservice-migration"
          imagePullPolicy: IfNotPresent
          name: migration
          env:
            
            - name: CHROMA_DB_LOG_URL
              # TODO properly use flow control here to check which type of value we need.

              value: "postgresql://chroma:chroma@postgres.chroma.svc.cluster.local:5432/log?sslmode=disable"
---
# Source: distributed-chroma/templates/sysdb-migration.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: sysdb-migration
  namespace: chroma
spec:
  template:
    metadata:
      labels:
        app: sysdb-migration
    spec:
      restartPolicy: OnFailure
      containers:
        - command:
            - "/bin/sh"
            - "-c"
            - "atlas migrate apply --url postgres://chroma:chroma@postgres:5432/sysdb?sslmode=disable"
          image: "local:sysdb-migration"
          imagePullPolicy: IfNotPresent
          name: migration
          env:
            
            - name: OPTL_TRACING_ENDPOINT
              # TODO properly use flow control here to check which type of value we need.

              value: "http://jaeger:4317"
---
# Source: distributed-chroma/templates/compaction-service-memberlist-cr.yaml
# These kubernetes manifests are UNDER ACTIVE  DEVELOPMENT and are not yet ready for production use.
# They will be used for the upcoming distributed version of chroma. They are not even ready
# for testing yet. Please do not use them unless you are working on the distributed version of chroma.

apiVersion: chroma.cluster/v1
kind: MemberList
metadata:
  name: compaction-service-memberlist
  namespace: chroma
spec:
  members:
---
# Source: distributed-chroma/templates/query-service-memberlist-cr.yaml
# These kubernetes manifests are UNDER ACTIVE  DEVELOPMENT and are not yet ready for production use.
# They will be used for the upcoming distributed version of chroma. They are not even ready
# for testing yet. Please do not use them unless you are working on the distributed version of chroma.

apiVersion: chroma.cluster/v1
kind: MemberList
metadata:
  name: query-service-memberlist
  namespace: chroma
spec:
  members:
